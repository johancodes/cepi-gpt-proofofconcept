{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"CEPI-gpt\" Proof of Concept\n",
    "\n",
    "Why do this?\n",
    "\n",
    "Data is not a business asset unless it is actually used. A ChatGPT-like experience is one way to encourage adoption of data to inform and accelerate work - through a simple and accessible natural language query on your data.\n",
    "\n",
    "What do we want to evidence?\n",
    "\n",
    "We want to create a ChatGPT-like experience to query and get answers from a corpus of first-party data. Technically we want to learn how word embeddings, vector databases and natural language queries can all come together to create this application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma # note capital letter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import OpenAI \n",
    "#from langchain import OpenAI, VectorDBQA # VectorDBQA is deprecated, instead import RetrievalQA. \n",
    "# See dev notes.\n",
    "from langchain.chains import RetrievalQA \n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "import os\n",
    "import nltk\n",
    "import magic\n",
    "\n",
    "# the below is used to manage the API keys in the .env\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from inside the .env file\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# if not using .env for some reason, use: os.environ[\"OPENAI_API_KEY\"] = \"KEY\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('/Users/joellim/Desktop/coda_work_mac_2022/openai-project-3/embeddings1', 'cepi100days.txt')\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1220, which is longer than the specified 1000\n",
      "Created a chunk of size 1628, which is longer than the specified 1000\n",
      "Created a chunk of size 1208, which is longer than the specified 1000\n",
      "Created a chunk of size 1198, which is longer than the specified 1000\n",
      "Created a chunk of size 1040, which is longer than the specified 1000\n",
      "Created a chunk of size 1213, which is longer than the specified 1000\n",
      "Created a chunk of size 1186, which is longer than the specified 1000\n",
      "Created a chunk of size 1542, which is longer than the specified 1000\n",
      "Created a chunk of size 1227, which is longer than the specified 1000\n",
      "Created a chunk of size 1149, which is longer than the specified 1000\n",
      "Created a chunk of size 1093, which is longer than the specified 1000\n",
      "Created a chunk of size 1359, which is longer than the specified 1000\n",
      "Created a chunk of size 1039, which is longer than the specified 1000\n",
      "Created a chunk of size 1018, which is longer than the specified 1000\n",
      "Created a chunk of size 1182, which is longer than the specified 1000\n",
      "Created a chunk of size 1045, which is longer than the specified 1000\n",
      "Created a chunk of size 1142, which is longer than the specified 1000\n",
      "Created a chunk of size 1222, which is longer than the specified 1000\n",
      "Created a chunk of size 1099, which is longer than the specified 1000\n",
      "Created a chunk of size 1149, which is longer than the specified 1000\n",
      "Created a chunk of size 1026, which is longer than the specified 1000\n",
      "Created a chunk of size 1041, which is longer than the specified 1000\n",
      "Created a chunk of size 1181, which is longer than the specified 1000\n",
      "Created a chunk of size 1562, which is longer than the specified 1000\n",
      "Created a chunk of size 1170, which is longer than the specified 1000\n",
      "Created a chunk of size 1881, which is longer than the specified 1000\n",
      "Created a chunk of size 1918, which is longer than the specified 1000\n",
      "Created a chunk of size 1090, which is longer than the specified 1000\n",
      "Created a chunk of size 1930, which is longer than the specified 1000\n",
      "Created a chunk of size 1116, which is longer than the specified 1000\n",
      "Created a chunk of size 1147, which is longer than the specified 1000\n",
      "Created a chunk of size 1141, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "doc_texts = char_text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#openAI_embeddings = OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "openAI_embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vStore = Chroma.from_documents(doc_texts, OpenAIEmbeddings)\n",
    "\n",
    "vStore = Chroma.from_documents(doc_texts, openAI_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=vStore.as_retriever())\n",
    "\n",
    "#model = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=vStore)\n",
    "\n",
    "# when I ran: model = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=vStore) \n",
    "# I got the following warning message:  /Users/joellim/miniforge3/envs/tf_m1/lib/python3.8/site-packages/langchain/chains/retrieval_qa/base.py:201: UserWarning: `VectorDBQA` is deprecated - please use `from langchain.chains import RetrievalQA`\n",
    "# So I researched and found this: https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A new study identified 37 innovations that enabled the accelerated development and authorisation of #COVID19 vaccines. It found that leveraging pre-existing insights, multiple processes running in parallel, and collaboration between stakeholders were key factors. #VaccinesWork\n"
     ]
    }
   ],
   "source": [
    "#query = 'what is the fundamental shift towards preparedness?'\n",
    "query = 'write a tweet about Key areas of innovation contributing to accelerated development and authorisation of COVID-19 vaccines'\n",
    "#query = 'Whats the link for \"Facts and figures: Ending violence against women\"?'\n",
    "#query = 'Who wrote \"Applying lessons from the Ebola vaccine experience for SARS-CoV-2 and other epidemic pathogens\"?'\n",
    "#query = 'summarize the 100-day vaccine creation plan in less than 30 words'\n",
    "#query = 'What are the barriers to vaccine creation - as a 6 verse poem'\n",
    "#query = 'What are barriers to 100-day vaccine development?'\n",
    "#query = 'What is CEPI\\'s budget?' \n",
    "#query = 'What is the main mission of CEPI and how?'\n",
    "\n",
    "print(model.run(query))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To demonstrate the cognition of the model and accuracy of answers, the answers to the following questions were correct and from the following pages.\n",
    "\n",
    "Q: 'what is the fundamental shift towards preparedness?'\n",
    "A: Page 5\n",
    "\n",
    "Q: 'write a tweet about Key areas of innovation contributing to accelerated development and authorisation of COVID-19 vaccines'\n",
    "A: Page 4\n",
    "\n",
    "Q: 'Whats the link for \"Facts and figures: Ending violence against women\"?'\n",
    "A: Page 62\n",
    "\n",
    "Q: 'Who wrote \"Applying lessons from the Ebola vaccine experience for SARS-CoV-2 and other epidemic pathogens\"?'\n",
    "A: Page 63\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask model to show where it got its answer from in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_for_source = 'Whats the link for \"Facts and figures: Ending violence against women\"?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_source_doc_return = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=vStore.as_retriever(), return_source_documents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='58', metadata={'source': '/Users/joellim/Desktop/coda_work_mac_2022/openai-project-3/embeddings1/cepi100days.txt'}),\n",
       " Document(page_content='42', metadata={'source': '/Users/joellim/Desktop/coda_work_mac_2022/openai-project-3/embeddings1/cepi100days.txt'}),\n",
       " Document(page_content='40', metadata={'source': '/Users/joellim/Desktop/coda_work_mac_2022/openai-project-3/embeddings1/cepi100days.txt'}),\n",
       " Document(page_content='2\\tAllen, T. et al., 2017. Global hotspots and correlates of emerging zoonotic diseases. Nature Communications, p. 8:1124. 3\\tThe concept of a stringent regulatory authority or SRA was developed by the WHO Secretariat and the Global Fund to Fight AIDS, Tuberculosis and Malaria to guide medicine procurement decisions and is now widely recognized by the international regulatory and procurement community. A list of stringent regulatory authorities can be consulted on: https://www.who.int/initiatives/who-listed-authority-reg-authorities/SRAs 4\\tWHO, 2022. WHO Coronavirus (COVID-19) Dashboard. 5\\tGopinath, G., 2020. A Long, Uneven and Uncertain Ascent. 6\\tUNESCO, 2021. UNESCO figures show two thirds of an academic year lost on average worldwide due to Covid-19 school closures. 7 UNWOMEN, 2022. Facts and figures: Ending violence against women. 8 The World Bank, 2020. COVID-19 to Add as Many as 150 Million Extreme Poor by 2021.', metadata={'source': '/Users/joellim/Desktop/coda_work_mac_2022/openai-project-3/embeddings1/cepi100days.txt'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_doc_return = model_source_doc_return({'query': query_for_source})\n",
    "\n",
    "source_doc_return['source_documents']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEV NOTES\n",
    "\n",
    "1. I had to install a bunch of new libraries: langchain, nltk (natural language tool kit), etc.\n",
    "\n",
    "2. VectorDBQA is deprecated. So 'from langchain.chains import RetrievalQA'. \n",
    "\n",
    "3. model = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=vStore)\n",
    "\n",
    "    - When I ran: model = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=vStore), \n",
    "    I got the following warning message: /Users/joellim/miniforge3/envs/tf_m1/lib/python3.8/site-packages/langchain/chains/retrieval_qa/base.py:201: UserWarning: `VectorDBQA` is deprecated - please use `from langchain.chains import RetrievalQA`\n",
    "    - So I researched and found this: https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html \n",
    "\n",
    "4. For the 'magic' module ('import magic') I first had to install libmagic with 'brew install libmagic' via terminal (within the tf_m1 environment).\n",
    "\n",
    "5. Does the model get information from outside the corpus i.e. from ChatGPT's knowledge base? When asked \"query = 'Which countries fund CEPI?'\" at times it answers \"I don't know\" and at other times it gives an answer like \" CEPI is funded by multiple countries, including the UK, Germany, Norway, Japan, and Canada.\" The source doc I used does not mention some of the nations - e.g. Canada, Norway, Germany. Baffling. When asked which nations founded CEPI it answered \"CEPI was founded by the governments of Germany, Norway, Japan, Canada, France, India, Italy, the Netherlands, the United Kingdom and the Bill & Melinda Gates Foundation.\" I checked the CEPI website and the correct answer is \"CEPI was founded in Davos by the governments of Norway and India, the Bill & Melinda Gates Foundation, Wellcome, and the World Economic Forum.\" At other times it answered \"I don't know\". All this to say, I suspect it is possible that the model sometimes might use data outside of the corpus or vector database to answer a query and users should check for veracity if this is ever suspected to be happening. However, the general accuracy of the answers are indeed impressive for this ChatGPT-like experience. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
